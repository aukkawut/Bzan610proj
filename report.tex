\documentclass[12pt]{article}
\usepackage{booktabs}
\usepackage{amssymb,amsmath, amsthm, endnotes,setspace,enumerate,ulem,color,xspace,lscape}
\usepackage{amsthm,subfigure,graphics,epsfig, hyperref}
\usepackage[sort]{natbib}
\allowdisplaybreaks[1]
\normalem
\newcommand{\textR}[1]{\textcolor{blue}{\texttt{#1}}}
\newcommand{\R}{\textR{R}}
\newtheorem{post}{Postulate}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\begin{document}

\title{Adversarial Pheromone-based Swarm Robots Dynamics as System of Stochastic Partial Differential Equations: Continuous-time Information Design Perspective}  

\author{
Aukkawut Ammartayakun   \thanks{Bredesen Center for Interdisciplinary Research and Graduate Education, University of Tennessee, Knoxville.  \\Email: {\tt aammartayakun@tennessee.edu}}  
}

\maketitle

\begin{center}
\textbf{Abstract}
\end{center}

In the foraging task, the goal for agents are to reach the objective and to accumulate said objective by bringing those back to the start. However, as shown by Aswale et al. (2022), if the information is communicate through the pheromone, then there exists the adversarial strategy that can sabotage the task. In this work, the theoretical framework for the adversarial foraging task will be presented along with the insights from the information theory perspective. System of stochastic partial differential equations is used to formulate the framework along with mean-field theory. 
\noindent
\textbf{Keywords}: Byzantine Model, Foraging Task, Swarm Robotics, Adversarial Behavior, Pheromone-based Robots

\thispagestyle{empty}

\newpage

\pagenumbering{arabic} % add page numbers to your report

\section{Introduction}\label{intro}


\section{Methodology}
\subsection{Autoencoder As a Pair of Parameterized Functions}
\begin{definition}[Autoencoder \citep{ae}]
Let say that we have a dataset which is $\mathbb{R}^{n} \supseteq \mathcal{D} = \left\{\mathbf{x}_1,\dots,\mathbf{x}_m\right\}$. Define the parameterized function that captures the low-dimensional representation $\phi_{\boldsymbol{\theta}_1}^{e}:\mathbb{R}^n\rightarrow \mathbb{R}^{p}$ where $p \neq n$ and $\boldsymbol{\theta}_1$ as parameters. The autoencoder with parameter $\boldsymbol{\Theta} = \begin{bmatrix}
    \boldsymbol{\theta}_1 & \boldsymbol{\theta}_2
\end{bmatrix}^\top$
is the combination (or composition) of the encoding parameterized function $\phi_{\boldsymbol{\theta}_1}^e$ and the decoding parameterized function $\phi_{\boldsymbol{\theta}_2}^{d}: \mathbb{R}^p\rightarrow\mathbb{R}^n$ specifically,
$$
\text{AE}(\mathbf{x}) = \phi_{\boldsymbol{\theta}_2}^d \circ \phi_{\boldsymbol{\theta}_1}^e \; (\mathbf{x})$$
\end{definition}

\subsection{Linear Models As Parameterized Functions}
\subsubsection{PCA and Least Square MMLR as an Autoencoder}

The goal of autoencoder is to perform dimensional reduction (from encoder) and the reconstruction of the information (from decoder). The basic linear model that can perform dimensional reduction would be the principal component analysis (PCA), and the basic linear model that can reconstruct the information would be the multiple multivariate linear regression (MMLR) which can be obtained using the least square method.
\begin{theorem}\label{thm:1}
Autoencoder, where the encoder is PCA and the decoder is least square MMLR, results in the reconstruction in the space constructed from the span of the principal component score.
\end{theorem}
\begin{proof}
    Let $\mathbf{X}\in \mathbb{R}^{n\times p}$ be a centered data matrix. $k$-rank approximation of $\mathbf{X}$ through PCA is \begin{align*}
        \tilde{\mathbf{X}} &= \mathbf{U}_k \mathbf{D}_k \mathbf{V}^\top_k
    \end{align*}
Least square estimate for $\mathbf{B}$ of the linear regression model 
$\mathbf{X} = \tilde{\mathbf{X}}\mathbf{B}$
is $$\mathbf{B} = \left(\tilde{\mathbf{X}}^\top\tilde{\mathbf{X}}\right)^{-1}\tilde{\mathbf{X}}^\top \mathbf{X}$$ 
Substitute in the $\mathbf{U}_k \mathbf{D}_k \mathbf{V}^\top_k$ to get
\begin{align*}
    \mathbf{B} &=\left((\mathbf{U}_k \mathbf{D}_k \mathbf{V}^\top_k)^\top \mathbf{U}_k \mathbf{D}_k \mathbf{V}^\top_k\right)^{-1} (\mathbf{U}_k \mathbf{D}_k \mathbf{V}^\top_k)^\top \mathbf{X}\\
    &= \left(\mathbf{V}_k \mathbf{D}_k \mathbf{U}^\top_k\mathbf{U}_k \mathbf{D}_k \mathbf{V}^\top_k\right)^{-1}\mathbf{V}_k \mathbf{D}_k \mathbf{U}^\top_k\mathbf{X}\\
    &= \left(\mathbf{V}_k \mathbf{D}_k\mathbf{D}_k \mathbf{V}^\top_k\right)^{-1}\mathbf{V}_k \mathbf{D}_k \mathbf{U}^\top_k\mathbf{X}\\
    &= \left(\mathbf{V}_k \mathbf{D}^2_k \mathbf{V}^\top_k\right)^{-1}\mathbf{V}_k \mathbf{D}_k \mathbf{U}^\top_k\mathbf{X}\\
    &= \left(\mathbf{V}_k \mathbf{D}^{-2}_k \mathbf{V}^\top_k\right)\mathbf{V}_k \mathbf{D}_k \mathbf{U}^\top_k\mathbf{X}\\
    &= \mathbf{V}_k \mathbf{D}_k^{-1}\mathbf{U}^\top_k \mathbf{X}
\end{align*}
The reconstruction then be 
\begin{align*}
    \tilde{\mathbf{X}}\mathbf{B} &= \mathbf{U}_k \mathbf{D}_k \mathbf{V}^\top_k\mathbf{V}_k\mathbf{D}_k^{-1}\mathbf{U}^\top_k \mathbf{X}\\ 
    &= \mathbf{U}_k\mathbf{U}_k^\top \mathbf{X}
\end{align*}
Since $\mathbf{U}_k\mathbf{U}_k^\top$ is a projection matrix, the reconstruction is a projection of the original data into the space spanned by the PC score.
\end{proof}

The result here shows that while the reconstructed image here would lie in the original space, it is only possible to get the image that is a span of the subspace defined by the $k$ basis from the PC score instead of the $p$ basis on which the original space is defined. Although the loss of information from this composition mapping is to be expected, the fact that the output is only restricted within this space is problematic in the generative task.





\end{document}


