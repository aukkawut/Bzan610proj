\documentclass[12pt]{article}
\usepackage{booktabs}
\usepackage{amssymb,amsmath, amsthm, endnotes,setspace,enumerate,ulem,color,xspace,lscape}
\usepackage{amsthm,subfigure,graphics,epsfig, hyperref}
\usepackage[sort]{natbib}
\allowdisplaybreaks[1]
\normalem
\newcommand{\textR}[1]{\textcolor{blue}{\texttt{#1}}}
\newcommand{\R}{\textR{R}}
\newtheorem{post}{Postulate}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\begin{document}

\title{Adversarial Pheromone-based Swarm Robots Dynamics as System of Stochastic Partial Differential Equations: Continuous-time Information Design Perspective}  

\author{
Aukkawut Ammartayakun   \thanks{Bredesen Center for Interdisciplinary Research and Graduate Education, University of Tennessee, Knoxville.  \\Email: {\tt aammartayakun@tennessee.edu}}  
}

\maketitle

\begin{center}
\textbf{Abstract}
\end{center}

Most of the time, neural networks are slow to train on the CPU. In this work, the concept of the linear model, which is faster to train on the CPU, as a substitution of the neural network for the case of an autoencoder and a variational autoencoder will be explored. Specifically on why one should and should not use the linear model as a generative model. Autoencoder can be defined as the composition between two mappings: encoding, which projects the high-dimensional data into the lower-dimensional manifold, and the decoder, which maps the point in the lower-dimensional manifold to the data. Basic models like principal component analysis (PCA) and least square multiple multivariate linear regression (MMLR) are used to understand the behavior and mathematically show that the reconstruction space is a span of the principal component score, which shows the tradeoff that while it is optimal in the least square sense, it is restrictive in the expression. The extension for the least square estimation to Bayesian methods like variational inference (VI) and Markov chain Monte Carlo (MCMC) method are used to estimate the parameter, hoping to widen the output space. The inference results show that the linear model is comparable in generating the image compared to the conditioned fully-connected variational autoencoder (CVAE).

\noindent
\textbf{Keywords}: Variational Autoencoder, Linear Regression, Principal Component Analysis, Bayesian Regression, Generative Model

\thispagestyle{empty}

\newpage

\pagenumbering{arabic} % add page numbers to your report

\section{Introduction}\label{intro}

Generative models in machine learning are dominated by non-linear architectures, particularly those based in deep learning \citep{cvae, gan, score, denoise}. However, these models often come with the drawback of computational intensity, especially when operated on CPUs. This study proposes an exploration into the utility of linear models within the framework of autoencoders, a class of generative models. Linear models are known for their computational simplicity and speed, but their capacity in generative tasks is not common \citep{elbo}.

Our investigation seeks to address a fundamental question: Can linear models provide a close approximation to the performance of deep learning models in generative tasks? To answer this, we implement a autoencoder where both the encoder and decoder are linear in nature. We utilize linear methods, such as Principal Component Analysis (PCA) and multiple multivariate linear regression (MMLR), to construct and analyze these linear autoencoders. The empirical focus is initially placed on the MNIST dataset \citep{mnist}, to assess the viability and performance of our linear approach.

The subsequent sections of this paper will delve into the technical aspects of our approach, present initial empirical results, discuss the mathematical interpretations, and finally, explore the broader implications and potential of linear generative models in the field of machine learning.
 
\section{Methodology}
\subsection{Autoencoder As a Pair of Parameterized Functions}
\begin{definition}[Autoencoder \citep{ae}]
Let say that we have a dataset which is $\mathbb{R}^{n} \supseteq \mathcal{D} = \left\{\mathbf{x}_1,\dots,\mathbf{x}_m\right\}$. Define the parameterized function that captures the low-dimensional representation $\phi_{\boldsymbol{\theta}_1}^{e}:\mathbb{R}^n\rightarrow \mathbb{R}^{p}$ where $p \neq n$ and $\boldsymbol{\theta}_1$ as parameters. The autoencoder with parameter $\boldsymbol{\Theta} = \begin{bmatrix}
    \boldsymbol{\theta}_1 & \boldsymbol{\theta}_2
\end{bmatrix}^\top$
is the combination (or composition) of the encoding parameterized function $\phi_{\boldsymbol{\theta}_1}^e$ and the decoding parameterized function $\phi_{\boldsymbol{\theta}_2}^{d}: \mathbb{R}^p\rightarrow\mathbb{R}^n$ specifically,
$$
\text{AE}(\mathbf{x}) = \phi_{\boldsymbol{\theta}_2}^d \circ \phi_{\boldsymbol{\theta}_1}^e \; (\mathbf{x})$$
\end{definition}

\subsection{Linear Models As Parameterized Functions}
\subsubsection{PCA and Least Square MMLR as an Autoencoder}

The goal of autoencoder is to perform dimensional reduction (from encoder) and the reconstruction of the information (from decoder). The basic linear model that can perform dimensional reduction would be the principal component analysis (PCA), and the basic linear model that can reconstruct the information would be the multiple multivariate linear regression (MMLR) which can be obtained using the least square method.
\begin{theorem}\label{thm:1}
Autoencoder, where the encoder is PCA and the decoder is least square MMLR, results in the reconstruction in the space constructed from the span of the principal component score.
\end{theorem}
\begin{proof}
    Let $\mathbf{X}\in \mathbb{R}^{n\times p}$ be a centered data matrix. $k$-rank approximation of $\mathbf{X}$ through PCA is \begin{align*}
        \tilde{\mathbf{X}} &= \mathbf{U}_k \mathbf{D}_k \mathbf{V}^\top_k
    \end{align*}
Least square estimate for $\mathbf{B}$ of the linear regression model 
$\mathbf{X} = \tilde{\mathbf{X}}\mathbf{B}$
is $$\mathbf{B} = \left(\tilde{\mathbf{X}}^\top\tilde{\mathbf{X}}\right)^{-1}\tilde{\mathbf{X}}^\top \mathbf{X}$$ 
Substitute in the $\mathbf{U}_k \mathbf{D}_k \mathbf{V}^\top_k$ to get
\begin{align*}
    \mathbf{B} &=\left((\mathbf{U}_k \mathbf{D}_k \mathbf{V}^\top_k)^\top \mathbf{U}_k \mathbf{D}_k \mathbf{V}^\top_k\right)^{-1} (\mathbf{U}_k \mathbf{D}_k \mathbf{V}^\top_k)^\top \mathbf{X}\\
    &= \left(\mathbf{V}_k \mathbf{D}_k \mathbf{U}^\top_k\mathbf{U}_k \mathbf{D}_k \mathbf{V}^\top_k\right)^{-1}\mathbf{V}_k \mathbf{D}_k \mathbf{U}^\top_k\mathbf{X}\\
    &= \left(\mathbf{V}_k \mathbf{D}_k\mathbf{D}_k \mathbf{V}^\top_k\right)^{-1}\mathbf{V}_k \mathbf{D}_k \mathbf{U}^\top_k\mathbf{X}\\
    &= \left(\mathbf{V}_k \mathbf{D}^2_k \mathbf{V}^\top_k\right)^{-1}\mathbf{V}_k \mathbf{D}_k \mathbf{U}^\top_k\mathbf{X}\\
    &= \left(\mathbf{V}_k \mathbf{D}^{-2}_k \mathbf{V}^\top_k\right)\mathbf{V}_k \mathbf{D}_k \mathbf{U}^\top_k\mathbf{X}\\
    &= \mathbf{V}_k \mathbf{D}_k^{-1}\mathbf{U}^\top_k \mathbf{X}
\end{align*}
The reconstruction then be 
\begin{align*}
    \tilde{\mathbf{X}}\mathbf{B} &= \mathbf{U}_k \mathbf{D}_k \mathbf{V}^\top_k\mathbf{V}_k\mathbf{D}_k^{-1}\mathbf{U}^\top_k \mathbf{X}\\ 
    &= \mathbf{U}_k\mathbf{U}_k^\top \mathbf{X}
\end{align*}
Since $\mathbf{U}_k\mathbf{U}_k^\top$ is a projection matrix, the reconstruction is a projection of the original data into the space spanned by the PC score.
\end{proof}

The result here shows that while the reconstructed image here would lie in the original space, it is only possible to get the image that is a span of the subspace defined by the $k$ basis from the PC score instead of the $p$ basis on which the original space is defined. Although the loss of information from this composition mapping is to be expected, the fact that the output is only restricted within this space is problematic in the generative task.





\end{document}


